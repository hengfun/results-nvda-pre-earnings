# Research Query: Latest data on the number of open-source AI models that are optimized for non-NVIDIA hardware.
**Generated:** Wednesday, August 27, 2025 at 12:06:38 PM
**Model:** gemini-2.5-flash

## Search Queries Used
- open-source AI models optimized for non-NVIDIA hardware last 3 months
- AMD Instinct open-source AI model optimization last 3 months
- Intel Gaudi open-source AI model optimization last 3 months
- number of open-source AI models supporting AMD GPUs last 3 months
- number of open-source AI models supporting Intel AI accelerators last 3 months
- challenges for open-source AI models on non-NVIDIA hardware last 3 months
- growth of non-NVIDIA optimized open-source AI models last 3 months

## Sources Referenced
- [amd.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGLAxIooL1eQzd5bhOXTqLe5M_P65xapy6xa0lljm1kTc3IKumlXZISqBL6i9zUfFOB3obBCFNQYRJIlwEX839VflHjf8ygu5WWamGYR-LLeB4pFboKhKETD6vssa_4uB_WzWHATyD-juBuxmyDNB3rsmV4uijTxGeJbFhxDSDLUs-c7Mw3QrEP-tsSwTpW5JVbfel-uNUMzaZYFqY7zKTzHnio6Bz7uKw4-w==)
- [github.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHxYJyQKFdg0qy3StJLv8clYfoCaHuCziBzWRjdpBiq1jrATs4bWacdaOyV4QA_kfb1Bn37tEpnPPdwB8Jpm_7BCLSqDRkikz4ytK76vF5HQLT2oME0TUYvZi687Mbna2PIPY7aYSw=)
- [github.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHq0vx2XtzBJAtSgFLdZyfgi7PVYktWRdzqqFjofEARHvmy3cQi2DDi62naeZXxc9Buduit9TnS-A3Mz7RV-Z_PjIKXsXbYwmmetBhUNAeakGitP410CSwgeQUrPe3UaqiyRvTnng==)
- [github.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHDvr582ZkaoxZB-U6zeOGUgntXl7XKynSNShNEwKVFkE51byPjtTw58_G9dtUoCU9h_9laO37BCWnXONzFdHny7fenIpF-5eZLSqKhmulN0hbaszaA4tXWZDNplOrs3yo=)
- [investingnews.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHSFPkSyEdf4xBeXgRy892aiD-QWJX94gegZVJ8bVpj-uzY2GfH2Wpl5dft1D6xOO4XxD7JYx8qM7_S5SQV2D8_4sIjDNfgAo7EebXSzV_MhAPATYctsMm6egqwGo9idfgXZcNA3LFwLq31dliR7wvZtI9LkFFFrgfq7bafB5lEaurXtRlweextpZrE7jpb)
- [nvidia.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHPj1wwlcmbEZVyDU65_XIq3fUsIPuCGetuWGfC7IPmQSJ7jqtTspYgAalT7FYjBFztzWPHRihJqrB6ufA4cDI8eo_xV2vvvmK9YfTpk2tqppst3KzDaCEkXssox4QJaAEMqIDkuRWcUlYC-mjzx58bFiN6Li4PCdL8grhTzTdlbiYNcghuXms88MyuoMU6DDZT6MYDO74ri0SP2SQ6MBgR6jLA_Ahu)

---

## Analysis

As a specialized pre-earnings research analyst for NVIDIA (NVDA), the objective is to identify material, non-consensus information regarding the optimization of open-source AI models for non-NVIDIA hardware, which could impact NVIDIA's Q2 FY2026 earnings report or forward-looking guidance. The current date is August 27, 2025, and NVIDIA reports earnings today.

Recent web searches, filtered for content published within the last three months, reveal a growing and increasingly capable ecosystem for open-source AI models on non-NVIDIA hardware, primarily driven by AMD and Intel. While NVIDIA remains a dominant player and is also actively contributing to the open-source community, the advancements by competitors, particularly in software optimization and strategic partnerships, suggest a potential for increased competition and diversification in the AI hardware market. This could lead to a gradual erosion of NVIDIA's perceived near-monopoly, especially in specific segments like inference or smaller-scale deployments, which might not be fully reflected in current market consensus.

Here are the key findings:

---

**Finding 1: AMD's Maturing Open AI Ecosystem and Production Workloads**

*   **Snippet:** "The latest version of the AMD open-source AI software stack, ROCm 7, is engineered to meet the growing demands of generative AI and high-performance computing workloads—while dramatically improving developer experience across the board. ROCm 7 features improved support for industry-standard frameworks, expanded hardware compatibility and new development tools, drivers, APIs and libraries to accelerate AI development and deployment. Microsoft announced Instinct MI300X is now powering both proprietary and open-source models in production on Azure. Cohere shared that its high-performance, scalable Command models are deployed on Instinct MI300X, powering enterprise-grade LLM inference with high throughput, efficiency and data privacy. Red Hat described how its expanded collaboration with AMD enables production-ready AI environments, with AMD Instinct GPUs on Red Hat OpenShift AI delivering powerful, efficient AI processing across hybrid cloud environments. Today, seven of the 10 largest model builders and Al companies are running production workloads on Instinct accelerators. Among those companies are Meta, OpenAI, Microsoft and xAI, who joined AMD and other partners at Advancing AI, to discuss how they are working with AMD for AI solutions to train today's leading AI models, power inference at scale and accelerate AI exploration and development: Meta detailed how Instinct MI300X is broadly deployed for Llama 3 and Llama 4 inference. OpenAI CEO Sam Altman discussed the importance of holistically optimized hardware, software and algorithms and OpenAI's close partnership with AMD on AI infrastructure, with research and GPT models on Azure in production on MI300X, as well as deep design engagements on MI400 Series platforms."
*   **Date:** 2025-06-12
*   **Source:** AMD Unveils Vision for an Open AI Ecosystem, Detailing New Silicon, Software and Systems at Advancing AI 2025, AMD
*   **Impact:** High. This finding indicates significant traction for AMD's Instinct MI300X accelerators and ROCm 7 software stack with major AI players like Meta, OpenAI, and Microsoft, who are deploying both proprietary and open-source models in *production workloads*. The mention of "deep design engagements on MI400 Series platforms" suggests a long-term commitment and growing confidence in AMD's hardware. This directly challenges NVIDIA's dominance in high-end AI, particularly if these deployments scale.
*   **Consensus Check:** Overlooked. While AMD's efforts are generally known, the specific details of "seven of the 10 largest model builders and AI companies" running production workloads on Instinct accelerators, and the explicit mention of Meta deploying Llama 3 and Llama 4 inference, and OpenAI using MI300X for GPT models on Azure, might be underestimated by the broader market. This suggests a more substantial shift in the competitive landscape than commonly perceived.

**Finding 2: Intel's OpenVINO for Broad AI Inference Optimization**

*   **Snippet:** "OpenVINO™ supports the CPU, GPU, and NPU devices and works with models from PyTorch, TensorFlow, ONNX, TensorFlow Lite, PaddlePaddle, and JAX/Flax frameworks. It includes APIs in C++, Python, C, NodeJS, and offers the GenAI API for optimized model pipelines and performance. Directly integrate models built with transformers and diffusers from the Hugging Face Hub using Optimum Intel. Convert and deploy models without original frameworks. Broad Platform Compatibility: Reduce resource demands and efficiently deploy on a range of platforms from edge to cloud. OpenVINO™ supports inference on CPU (x86, ARM), GPU (Intel integrated & discrete GPU) and AI accelerators (Intel NPU)."
*   **Date:** Various updates within last 3 months (e.g., "last month", "2 months ago", "4 months ago" for specific file updates, indicating ongoing development).
*   **Source:** OpenVINO™ is an open source toolkit for optimizing and deploying AI inference - GitHub
*   **Impact:** Medium. Intel's OpenVINO toolkit provides a robust, open-source solution for optimizing and deploying AI inference across a wide range of Intel hardware (CPUs, integrated/discrete GPUs, NPUs) and popular frameworks. This broad compatibility, especially for edge and cloud deployments, offers a compelling alternative for cost-sensitive or diverse hardware environments, potentially impacting NVIDIA's inference market share in these segments.
*   **Consensus Check:** Widely known but underestimated. While OpenVINO is a recognized tool, its continuous development and growing integration with the open-source AI ecosystem, including direct integration with Hugging Face models, might not be fully appreciated in the context of NVIDIA's overall market valuation.

**Finding 3: DeepSeek-V3's Support for AMD and Huawei Hardware**

*   **Snippet:** "DeepSeek-V3 can be deployed locally using the following hardware and open-source community software: ... AMD GPU: Enables running the DeepSeek-V3 model on AMD GPUs via SGLang in both BF16 and FP8 modes. Huawei Ascend NPU: Supports running DeepSeek-V3 on Huawei Ascend devices in both INT8 and BF16."
*   **Date:** Content is current, with related SGLang updates in August and June 2025.
*   **Source:** deepseek-ai/DeepSeek-V3 - GitHub
*   **Impact:** Medium. DeepSeek-V3 is highlighted as a strong open-source model, comparable to leading closed-source models. Its explicit support for AMD GPUs (via SGLang) and Huawei Ascend NPUs, including high-performance BF16 and FP8 modes, demonstrates that top-tier open-source models are being actively optimized for non-NVIDIA hardware. This provides viable alternatives for users and enterprises seeking diverse hardware solutions.
*   **Consensus Check:** Overlooked. The specific and high-performance (BF16, FP8) support for a leading open-source model like DeepSeek-V3 on AMD and Huawei hardware might not be widely known or fully factored into the market's perception of NVIDIA's competitive moat.

**Finding 4: SGLang's Role in Enabling High-Performance LLM Serving on AMD**

*   **Snippet:** "[2025/08] SGLang x AMD SF Meetup on 8/22: Hands-on GPU workshop, tech talks by AMD/xAI/SGLang, and networking (Roadmap, Large-scale EP). [2025/06] SGLang, the high-performance serving infrastructure powering trillions of tokens daily, has been awarded the third batch of the Open Source AI Grant by a16z (a16z blog). [2025/03] Supercharge DeepSeek-R1 Inference on AMD Instinct MI300X (AMD blog); [2025/03] SGLang Joins PyTorch Ecosystem: Efficient LLM Serving Engine (PyTorch blog); [2025/01] SGLang provides day one support for DeepSeek V3/R1 models on NVIDIA and AMD GPUs with DeepSeek-specific optimizations."
*   **Date:** August 2025, June 2025, March 2025.
*   **Source:** sgl-project/sglang: SGLang is a fast serving framework for large language models and vision language models. - GitHub
*   **Impact:** Medium-High. SGLang is a crucial open-source framework for fast LLM serving. Its explicit and ongoing collaboration with AMD, including workshops and optimization for AMD Instinct MI300X for models like DeepSeek-R1/V3, signals a maturing and competitive software ecosystem for non-NVIDIA hardware. The recognition through an a16z Open Source AI Grant and integration into the PyTorch ecosystem further validates its importance.
*   **Consensus Check:** Overlooked. While SGLang is known in developer circles, its specific and impactful role in enabling high-performance LLM serving on AMD hardware and its rapid growth might not be fully appreciated by the broader investment community, which often focuses on hardware sales rather than underlying software infrastructure.

**Finding 5: IBM and AMD Collaboration for Enterprise AI**

*   **Snippet:** "Beyond high-performance computing, AMD CPUs, GPUs and open-source software also power numerous generative AI solutions for leading enterprises and cloud providers around the world. ... AMD and IBM are exploring how to integrate AMD CPUs, GPUs, and FPGAs with IBM quantum computers to efficiently accelerate a new class of emerging algorithms, which are outside the current reach of either paradigm working independently."
*   **Date:** 2025-08-26
*   **Source:** IBM and AMD Join Forces to Build the Future of Computing | INN
*   **Impact:** Medium. This very recent announcement of a collaboration between AMD and IBM, specifically highlighting AMD's open-source software and GPUs powering generative AI solutions for enterprises and cloud providers, reinforces the growing ecosystem around AMD. The partnership with a major enterprise technology provider like IBM could significantly expand AMD's reach into corporate AI deployments.
*   **Consensus Check:** Overlooked. While a partnership between two large companies might be noted, the specific implication for open-source AI models and the potential for AMD to gain significant traction in enterprise and cloud AI solutions, potentially at NVIDIA's expense, might be underappreciated.

**Finding 6: NVIDIA's Own Open-Source Contributions (Counterpoint)**

*   **Snippet:** "Open source AI models such as Cosmos, DeepSeek, Gemma, GPT-OSS, Llama, Nemotron, Phi, Qwen, and many more are the foundation of AI innovation. How do open source tools scale AI innovation? Accelerating AI requires more than powerful hardware and open source AI models—it demands an optimized and rapidly evolving software stack to deliver optimal performance for today's demanding AI workloads. NVIDIA is democratizing access to cutting-edge AI capabilities by releasing open source tools, models, and datasets for developers to innovate at the system level. You can find 1,000+ open source tools through NVIDIA GitHub repos, and the NVIDIA Hugging Face collections offer 450+ models and 80+ datasets."
*   **Date:** 2025-08-22
*   **Source:** NVIDIA Hardware Innovations and Open Source Contributions Are Shaping AI
*   **Impact:** Medium. This snippet, from NVIDIA itself, acknowledges the critical role of open-source AI models and highlights NVIDIA's substantial contributions to the open-source ecosystem (1,000+ tools, 450+ models on Hugging Face). This indicates that NVIDIA is actively engaged in and adapting to the open-source trend, which could mitigate the impact of competitors' open-source efforts by ensuring NVIDIA hardware remains a preferred platform even for open models.
*   **Consensus Check:** Widely known. NVIDIA's engagement with the open-source community is generally understood, but the specific scale of their contributions might be less known. This serves as an important counter-balance, showing NVIDIA is not ignoring the open-source movement.

---

**Contradictions and Gaps:**

*   **Contradictions:** No direct contradictions were found. All major players (AMD, Intel, NVIDIA) acknowledge the importance of open-source AI and are actively contributing to their respective ecosystems. The competition lies in market share and performance claims rather than conflicting factual information.
*   **Gaps:**
    *   **Quantitative Market Share Data:** Precise, independent data on the number or percentage of open-source AI models *primarily* optimized for non-NVIDIA hardware, or the market share of non-NVIDIA hardware in open-source AI deployments, is still lacking. The findings highlight *efforts* and *partnerships* but not comprehensive market shifts.
    *   **Independent Performance Benchmarks:** While AMD and Intel make performance claims against NVIDIA, these are often self-reported. Independent, comprehensive benchmarks across a wide range of open-source models and diverse hardware would provide a clearer, unbiased picture of the competitive landscape.
    *   **Direct Financial Impact:** The extracted information indicates growing competitive pressure and viable alternatives to NVIDIA. However, directly quantifying the financial impact on NVIDIA's upcoming earnings or forward-looking guidance without more specific data on order cancellations, market share loss, or pricing pressure is challenging.
    *   **Long-term Sustainability of Non-NVIDIA Software Stacks:** While ROCm and OpenVINO are maturing, the long-term developer mindshare and ease of use compared to NVIDIA's CUDA ecosystem remain a crucial factor that is difficult to assess purely from these snippets.