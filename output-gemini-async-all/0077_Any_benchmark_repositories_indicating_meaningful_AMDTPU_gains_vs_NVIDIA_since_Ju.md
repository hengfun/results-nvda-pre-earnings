# Research Query: Any benchmark repositories indicating meaningful AMD/TPU gains vs NVIDIA since June?,MLPerf submissions; academic benchmarks; HuggingFace evals,MLPerf AMD TPU gain vs NVIDIA since June 2025
**Generated:** Wednesday, August 27, 2025 at 11:58:38 AM
**Model:** gemini-2.5-flash

## Search Queries Used
- MLPerf AMD vs NVIDIA performance since June 2025
- MLPerf TPU vs NVIDIA performance since June 2025
- AMD Instinct MI300X benchmarks vs NVIDIA H100 since June 2025
- Google TPU v5e benchmarks vs NVIDIA H100 since June 2025
- HuggingFace AMD performance gains since June 2025
- HuggingFace TPU performance gains since June 2025
- academic benchmarks AMD vs NVIDIA AI since June 2025
- academic benchmarks TPU vs NVIDIA AI since June 2025
- AI accelerator performance gains Q2 2025 AMD TPU

## Sources Referenced
- [ainvest.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG7DylIhn2eCZtc7n7KRzEd6Z2nf7RJOx259x1a6A61zP8O8yGOwtUgMNrxv8xb1PpeEtSJnU1OksgpAdoFPLUmeVRvbh-t1yZbY-L3teo91rEAxj-jViKUkHSRzHh-uowvPEa0pCNuEb9k_INmcgQPUVj0uWAU6I29FjoK2R31WbnTWzo_FvfbZ-Bj7XMlEm9WtofBhF_HE-m5taBfIN_QYMX_fe-En9IG9-aNxiXwHimIs6TC1RszXib8WD9p)
- [moomoo.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQECR-OSZ2UsguMQfQXbLiAuhVFaqxXXN1JuHKgnZcgYN8mnw4Jk9yDIk-qBgwOXIqegFCGEBEDj6PtG8Ba0de765TgrZ2e4AjTgVhNDZmsTT_SiKNXLFfOqm1XW4-RRAFslvZSD4GQXESDOPoCKEzNBSsEMHfeXqbG7R4rzl-3i0NapyuTZsaj4GyPGb8102yRX__jPr0639LIR)
- [entrepreneur.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEKUHBW1q-pzq3bpUZ8nt-S5cOwWmMcagA7MmxpVlKfSg4LiAloB-bFfCHNn-vYPNP-z3s-T_zZc12b1MV75XTKQ-jI0u5SkteLSHWd1Sv0ktRpkKtmrVxm0SmIrioS0333p9hRQAuyQ7srHFp0mhXkG0v7bzPskPV6ohLFvvUsp_b_rh2ykkKHYa3zIjcLF1wF-egqCcCpMndbcw==)
- [nai500.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEgQ3YxIzMWe8zXquH9LWrqJxpmmopGahrvrnNKoZvXOrvbL5k9MMSbHiyk9Uovo6oiopjKwvAQnHShxAbJFy8u1SgWq9bx7cEYZd0yu5U3XtdLcIKGTPP8Q-qkQrlub24jNg5K5KNODJB-765E-aQYvrN3lICwK3NPihrNVxoHpQvGgpMluapJRv6zUVzavXkNzYIHRNWotpDmH-lDsvGIAcgC_vU=)
- [techwireasia.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH_dF0HbmaOlO8CuwLC7lxH0ltXjZCrO_UvsXZaN-2l9vjbtXxrdlxdHwIwJntYCzTDpep_h34kip8nTv5FEel7SLfVkq47X_zMXspwalhYDOb1C9pBLGiOtJqzADGzopqRQbM4iKLM-G0P10g8MUteyVtYA5lW3YwA6V1avAxQzq7oaLorZuADmZf7MnP-BrnBqgx--2Won4PCZ5W4vKk3CZzqVqs=)
- [techpowerup.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGz5C3-kcLEHpEC6hXy3rIB7KxpdO0hMSXKUX0Ms1AxzlHJZy3hIP1t_sdHpc6Mnpw5bZ6DH-qtlGlWHdjcAyA6O8GL0vhOSJU7e7WnhRYywMbDUMbQOlrQRhluQsDgSdoIBNYJ2073evG135TRVT6hDvTuJ8T6hyRI2CXEv07N2NTaASlPVvivbm5mJ0ell75wIOZhrAY2eHJKv2bncmQ=)
- [semianalysis.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFcJC8Xw79HajbD_zv1luDsYiYUYKM8A8MWJRlURGaOr-rVfgGJKIzwdTH24MfsHq4_FUMXjXavn8jUrGgPm4oUap6lE_HUBk93z4HIe9xDbg_Wx02RdWkpbQEExEZrRTK8qRJ3FE-sk1y4vCon4v_qH_PBX1c6joAMSiiGVOl8a_KL-0ZPhOdNCiWoHPbn1Ohegna8lWgpuANU505WHoPbnB1K0aBiQUF6Cfp3_Ro=)
- [forbes.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFvejpyJV2Cy2DMo3Ec5icWSt0opRAduTipAeIrU9Ux_zPSVpZZtLutOz-u66HWdA5Acwl8b4QF_LmGRh4Zpc7dlVsTE9D6MMoh1cvcP3V93vCxN7PaaVB2aK4ABjP5mV_z9dwGdUCAi97wjl4mkPcgqoKufNYrHdW_7NJE5tOOJpEqPPQydUDRCqXihSdiy3EtJLhVeFuGRkXU_g0WRQhfdp4av-GmWA==)

---

## Analysis

As a specialized pre-earnings research analyst for NVIDIA (NVDA), I have identified several material, non-consensus data points regarding competitive dynamics from AMD and Google's TPUs, published within the last three months. These findings suggest a nuanced and potentially challenging landscape for NVIDIA, particularly concerning inference workloads and cost-efficiency, alongside some strong counter-arguments in NVIDIA's favor regarding software maturity and overall market share.

**Key Themes and Analysis:**

1.  **AMD's Aggressive Push and Performance Claims:** AMD's new MI350 series (MI350X, MI355X) is directly targeting NVIDIA's Blackwell architecture (B200/GB200) with claims of significant performance and cost-efficiency advantages, particularly in inference. AMD asserts up to 3x performance over its previous generation MI300X, 13% faster training times than NVIDIA's B200 in MLPerf 5.0, 30% more tokens per second than B200, and matching the GB200 in certain inference tasks. Crucially, AMD claims 40% better token-per-dollar performance compared to NVIDIA's GB200 and B200.

2.  **Hyperscaler Interest in AMD:** Wall Street analysts, specifically Truist Securities, report that some hyperscale clients are "seriously considering a moderate shift" from NVIDIA to AMD for more cost-effective and comprehensive AI computing systems. Oracle's deployment of a 27,000-node AI cluster powered by MI355X GPUs is cited as a real-world example of AMD's scalability.

3.  **Google TPU's Inroads with OpenAI:** OpenAI, a critical NVIDIA customer, began leasing Google Cloud's Tensor Processing Units (TPU v6e "Trillium") in June 2025 for ChatGPT's increasing *inference* workload. This marks OpenAI's first large-scale production reliance on non-NVIDIA chips. The move is driven by cost-effectiveness for steady-state inference and a strategic desire to reduce vendor lock-in, especially given past GPU supply shortages and price fluctuations. Inference accounts for nearly half of OpenAI's estimated $40 billion annual compute budget.

4.  **NVIDIA's Enduring Strengths and AMD's Challenges:** Despite AMD's claims, some analyses highlight NVIDIA's continued dominance and AMD's weaknesses. NVIDIA still holds a vast majority of the AI chip market share (92% in Q1 2025 according to Jon Peddie Research, 85.2% according to IDC). Furthermore, a SemiAnalysis report from May 2025 points to significant software maturity issues for AMD's ROCm, claiming "worse accuracy quality on AMD when compared to using NVIDIA" and "25% of the tested models are failing accuracy tests." This report also suggests AMD's market share dipped in Q1 2025 and is expected to decline in Q2 2025 due to NVIDIA's Blackwell ramp and delays in AMD's MI325X shipments. The latest MLPerf results (June 4, 2025) show AMD catching up with NVIDIA's *older* H200 GPU (8% better on Llama 2-70B LORA training) but explicitly state AMD "cannot compete with Blackwell today" and lacks a competitive networking solution like NVLink.

**Contradictions and Gaps:**
There is a notable contradiction between AMD's aggressive performance and cost-efficiency claims for its MI350 series against Blackwell (B200/GB200) and the more cautious or negative assessments from third-party analyses (Forbes, SemiAnalysis) regarding AMD's ability to compete with Blackwell and its software stack. The specific impact of these conflicting claims on customer purchasing decisions and NVIDIA's actual sales remains a key unknown. While there's good information from MLPerf and company claims, detailed, independent academic benchmarks or HuggingFace evaluations directly comparing recent AMD/TPU gains against NVIDIA's latest offerings since June 2025 are still somewhat limited.

---

**Structured Findings:**

**Finding 1:**
-   **Snippet:** "Benchmark test results show that the HBM3E memory capacity of the AMD MI355X is 1.5 times that of NVIDIA's GB200/B200 AI GPUs, and in terms of peak FP64/FP32 performance, the MI355X has approximately twice the advantage. In terms of inference throughput, which is a focus for AI engineers, based on Llama-3 405B inference, the MI355X (FP4) offers a 30% improvement in Token/s/$ compared to the Blackwell B200, and it remains on par with the GB200. AMD claims that in certain high-parameter AI training workloads, it leads by up to 1.13 times."
-   **Date:** August 26, 2025
-   **Source:** Moomoo (citing Truist Securities), URL not directly available.
-   **Impact:** High. Specific, strong performance and cost-efficiency claims for AMD's MI355X against NVIDIA's latest Blackwell chips (B200/GB200) in a critical inference workload (Llama-3 405B). This directly challenges NVIDIA's pricing power and performance leadership in key segments.
-   **Consensus Check:** AMD's official claims are public, but the specific "30% improvement in Token/s/$" against Blackwell B200 and "on par with GB200" for a key LLM inference workload, coupled with the analyst upgrade and client shift, could be non-consensus or at least underappreciated.

**Finding 2:**
-   **Snippet:** "Truist emphasized in the report that some hyperscale customers focused on AI infrastructure are now seriously considering a moderate shift from NVIDIA's (NVDA.US) AI server clusters to AMD to obtain AI computing solutions with better cost and comprehensive performance advantages. This suggests that NVIDIA's market share, which accounts for up to 90% in the AI chip and server cluster market, may gradually be eroded by AMD in the future."
-   **Date:** August 26, 2025
-   **Source:** Moomoo (citing Truist Securities), URL not directly available.
-   **Impact:** High. Direct indication from a Wall Street analyst firm that hyperscale customers are actively evaluating and considering shifting away from NVIDIA to AMD due to cost and performance. This is a significant threat to NVIDIA's dominant market share and could impact forward guidance.
-   **Consensus Check:** This specific insight into hyperscaler sentiment from a financial analyst firm is likely non-consensus and highly material.

**Finding 3:**
-   **Snippet:** "In June 2025, OpenAI began leasing Google Cloud's Tensor Processing Units to handle ChatGPT's increasing inference workload. This is the first time OpenAI has relied on non-NVIDIA chips in large-scale production. ... Inference operations account for nearly half of OpenAI's estimated $40 billion annual compute budget. Google's TPUs, like v6e "Trillium" provide a more cost-effective solution for steady-state inference, as they are designed specifically for high throughput and low latency. Beyond cost savings, this decision reflects OpenAI's desire to reduce reliance on any single vendor. Microsoft Azure has been its primary cloud provider since early investments and collaborations. However, GPU supply shortages and price fluctuations exposed a weakness in relying too heavily on a single source."
-   **Date:** June 30, 2025
-   **Source:** TechPowerUp, URL not directly available.
-   **Impact:** High. OpenAI, a major NVIDIA customer, diversifying to Google TPUs for large-scale inference is a significant development. It directly impacts NVIDIA's inference revenue stream and highlights a strategic move by a key AI player to reduce vendor lock-in and seek cost-effective alternatives, driven by past supply issues.
-   **Consensus Check:** While OpenAI's move might be known, the specific details about inference accounting for half of their $40B annual compute budget and the explicit reasons (cost-effectiveness, vendor lock-in, supply shortages) are critical and potentially underappreciated.

**Finding 4:**
-   **Snippet:** "AMD also set a new record in 2024, with revenue of $25.8 billion, up 24% year-over-year. The company had a market capitalization of $190 billion at the time of writing, compared to Nvidia's $3.49 trillion. Though Nvidia is the undisputed leader in AI chips, capturing over 80% of the market, AMD CEO Lisa Su says AMD's latest chips are "outperforming" Nvidia's with "greater efficiency." Su said at an AMD launch event on Thursday in San Jose, California, that AMD's new MI350 chips are up to 35 times faster than previous generations, per Bloomberg. The MI350 chips began shipping out earlier this month. ... When it comes to running AI programs, Su claims that AMD's MI355 chip offers "greater efficiency" compared to Nvidia's B200 and GB200 chips, which were released in 2024. She said that the MI355X chip "matches the performance of the significantly more expensive and complex [Nvidia] GB200" at a lower price point. ... OpenAI CEO Sam Altman made an appearance on stage with Su at the event to say that his company would use the latest AMD chips."
-   **Date:** June 13, 2025
-   **Source:** Entrepreneur (citing Bloomberg), URL not directly available.
-   **Impact:** High. Direct claims from AMD's CEO about MI355X matching GB200 performance at a lower price point, coupled with Sam Altman's endorsement, are very material. This directly challenges NVIDIA's pricing power and market dominance, especially with a key industry figure publicly supporting AMD.
-   **Consensus Check:** AMD's launch event and CEO claims are public. Sam Altman's appearance is also public. The specific performance/price claims and the high-profile endorsement are key details that could be non-consensus or not fully digested.

**Finding 5:**
-   **Snippet:** "AMD said the system can deliver 40% more tokens per dollar when compared to Nvidia B200 setups using proprietary software, thanks in part to support for open-source stacks like SGLang and vLLM."
-   **Date:** June 13, 2025
-   **Source:** Advancing AI 2025 event coverage, URL not directly available.
-   **Impact:** High. A very specific and strong claim of 40% better cost-efficiency ("tokens per dollar") for AMD's MI350 series against NVIDIA's B200, leveraging open-source software. This directly targets NVIDIA's total cost of ownership advantage and highlights the growing importance of open-source ecosystems.
-   **Consensus Check:** This is an official AMD claim from their launch event, so it's public. However, the exact figure and its direct comparison to Blackwell B200 on a cost-efficiency metric using open-source stacks are crucial details that might be non-consensus in terms of their market impact on NVIDIA.

**Finding 6:**
-   **Snippet:** "Using ROCm makes the models “Dumber” than on CUDA... For most models, we observe worse accuracy quality on AMD when compared to using NVIDIA. 25% of the tested models are failing accuracy tests when run on AMD. This means that using the same model on ROCm, you get dumber answers than what you would get on NVIDIA. AMD needs to task more 996 engineers to fix this immediately!"
-   **Date:** May 23, 2025
-   **Source:** SemiAnalysis, URL not directly available.
-   **Impact:** High. This is a *very strong negative* for AMD and a significant positive for NVIDIA. Accuracy issues are fundamental for AI workloads. If true, this represents a critical barrier to AMD adoption and underscores NVIDIA's software ecosystem advantage (CUDA). This is a non-consensus finding from a niche source.
-   **Consensus Check:** This is highly non-consensus and very material. Accuracy issues are a fundamental problem for AI workloads and if true, would be a significant barrier for AMD adoption, strongly favoring NVIDIA. SemiAnalysis is a niche publication, which aligns with the "non-consensus" requirement.

**Finding 7:**
-   **Snippet:** "AMD's market share in Datacenter AI GPUs has been increasing at a steady pace since Q1 CY2023. However, in Q1 CY2025, Nvidia's massive Blackwell ramp commenced, and with AMD's answer to Blackwell only coming by Q3 2025, AMD's market share accordingly dipped in Q1 CY2025. We expect AMD's market share to decline in Q2 CY2025."
-   **Date:** May 23, 2025
-   **Source:** SemiAnalysis, URL not directly available.
-   **Impact:** Medium. This indicates that NVIDIA's Blackwell ramp is already having a tangible impact on AMD's market share, leading to a decline in Q1 and an expected decline in Q2 2025. This is positive for NVIDIA's upcoming earnings report.
-   **Consensus Check:** While NVIDIA's Blackwell ramp is known, the specific impact of a *decline* in AMD's market share in Q1 and Q2 2025 due to this ramp is a specific, potentially non-consensus data point that would be favorable for NVIDIA's earnings.