# Research Query: MLPerf new submissions using GB200; perf/W gains for TCO assumptions.
**Generated:** Wednesday, August 27, 2025 at 12:02:46 PM
**Model:** gemini-2.5-flash

## Search Queries Used
- MLPerf new submissions GB200 perf/W gains TCO
- NVIDIA GB200 MLPerf results power efficiency
- GB200 TCO advantages MLPerf
- MLPerf H2 2025 submissions GB200
- NVIDIA GB200 energy efficiency benchmarks
- GB200 performance per watt MLPerf
- MLPerf inference GB200 new results
- MLPerf training GB200 new results
- NVIDIA GB200 customer TCO analysis
- GB200 competitive performance per watt

## Sources Referenced
- [semianalysis.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGG7EEV4FQ12GaijyuyAPuDEFcc8jmpxyBxbQauTJ0gm7gH0uV0GltcXV_asnTOQXgquMfucsfF4YRbd_NM9PQcSfjs5kgZUhYbVAkevOZkPMODLYxJF2AMfu3yLr9pZnLvqvaeG49b2YBBFYRLwJfmGsnpx3VdS-nNBT21HRTEi4TW8W4cwnk=)
- [hpcwire.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHcJHeZ1LbSABk7KT0J6nsLEbafST1WhGF4sQLQG1ul44tN5kfwo3S--ot-YQ-uOhXD7lT42oqnrPHdBEUf149heSBJKHiARUM0hu_kvxcnQgjaUbh-I5Le-U3JoDR3aj8_lFjmYvih0pCe4ADQy_vy0DjBZ3QmKAP2t9boSc0XodVi3rJQEZBBBmit55A867Vc9p9VOFptm-Z0YQQkQYhm)
- [aiwire.net](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG_tJFZAzrqATQiLRrs3YN_WjcfnAhGPaMDvBAbMsWtp_KywhNN8mDHBvx9w7G7otXPRlVEj8Qwbff0IgeUoLosqTGR34b4NHtkAndt5rLaArdqrUFZ3qf9dXNQwkCkauUpOhYi06XZSXPryPFoT3Wf_lrrWHhasrvi_V8BLPiTmcG46uN8QafzBXTnAo9Dquf6pH6zw4-7FVxvmgyxdZg=)
- [stocktitan.net](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF3O-mltVJX5m7tXDFd4Lz5EzHWSCjD5w94gJNgoydOBbRvGQjXxXz2iTswwZNMdfvX_8gNnq7555HLvy5aGtv3S2ilHaMyF3VLMU0iD4Od9FyWXbDUAxYKuMTUUYyj38KN_E6b8dezrilbFTZPgv-yX-S5CvNItRR92qMDADOTjQfncZS_glzliEzEMLj3mCP8RWkyWMKle7I5L0-_RLStrc-UQvM2mln6CaoQ9e4mUvqK)
- [continuumlabs.ai](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHHx0F5FxxmVbu1P2ov1IpU2kPnhG8p8ozWqfYFhQsjEKYr79G_0jDS8wgHAQtV4etwU_lieMNAJtiCwwiIbm7XTxcInbRHHAC4LpwJE96oV4nwt04vdtddMthyZpEKMe9edqKBKTbpmFI4xBYCa9zMPcaTCKsEBSczmRSgv4VHODKhGV_xAjoaCkCSrGifmOPXCFRaw7qs4K1ezYkj3-GCTV7-ijSv3Mty6MV_KM3XVGMHfw==)

---

## Analysis

NVIDIA's Q2 FY2026 earnings report arrives today amidst a backdrop of strong MLPerf benchmark results for its new Blackwell GB200 platform, yet recent analyst reports introduce material non-consensus information regarding the platform's immediate deployability and total cost of ownership (TCO). While MLPerf results highlight significant performance leaps, particularly in large-scale AI training and inference, a recent SemiAnalysis report suggests that large-scale GB200 deployments are currently hampered by software maturity and reliability challenges. This divergence between benchmark performance and real-world readiness could impact NVIDIA's near-term guidance and customer adoption rates for its most advanced AI systems.

**Key Findings:**

*   **Snippet:** "Currently there are no large-scale training runs done yet on GB200 NVL72 as software continues to mature and reliability challenges are worked through. This means that Nvidia's H100 and H200 as well as Google TPUs remain the only GPUs that are today being successfully used to complete frontier-scale training. As it stands today, even the most advanced operators at frontier labs and CSPs are not yet able to carry out mega training runs on the GB200 NVL72. With that said, every new architecture naturally requires time for the ecosystem to ramp software to effectively utilize the architecture. The GB200 NVL72 ramp is slightly slower than prior generations"
    *   **Date:** August 20, 2025
    *   **Source:** SemiAnalysis, "H100 vs GB200 NVL72 Training Benchmarks – Power, TCO, and Reliability Analysis, Software Improvement Over Time"
    *   **Impact:** High. This is highly material and non-consensus. It directly challenges the immediate revenue potential and widespread adoption of GB200 for the most demanding AI workloads, suggesting a slower ramp than anticipated despite strong benchmark numbers. This could lead to a more cautious outlook from management or a miss on aggressive analyst expectations for Blackwell's immediate impact.
    *   **Consensus Check:** Overlooked. MLPerf results (June 2025) indicate high performance and scalability, creating a consensus of immediate readiness. This report provides a critical counter-narrative.

*   **Snippet:** "When factoring in both capex and opex in order to arrive at the total cost of ownership (TCO), we see that TCO for the GB200 NVL72 is about 1.6x higher than TCO for the H100. This means that the GB200 NVL72 needs to be at least 1.6x faster than the H100 in order to have an performance per TCO advantage when compared to the H100."
    *   **Date:** August 20, 2025
    *   **Source:** SemiAnalysis, "H100 vs GB200 NVL72 Training Benchmarks – Power, TCO, and Reliability Analysis, Software Improvement Over Time"
    *   **Impact:** Medium. While GB200 offers significant performance gains, a 1.6x higher TCO suggests that customers will need to see substantial and consistent performance improvements to justify the investment. This could influence purchasing decisions and the pace of GB200 adoption, especially for customers sensitive to overall operational costs.
    *   **Consensus Check:** Overlooked. NVIDIA often highlights raw performance and energy efficiency, but detailed TCO comparisons that include capex and opex are less frequently emphasized in public disclosures, especially when they show a higher cost.

*   **Snippet:** "The cost difference comes from the fact the GB200 NVL72 has a higher all-in power consumption per GPU than the H100. This is primarily driven by the fact that the GB200 chip consumes 1200W per chip vs 700W for the H100."
    *   **Date:** August 20, 2025
    *   **Source:** SemiAnalysis, "H100 vs GB200 NVL72 Training Benchmarks – Power, TCO, and Reliability Analysis, Software Improvement Over Time"
    *   **Impact:** Medium. Increased power consumption per GPU for GB200 (1200W vs. 700W for H100) directly impacts operating expenses (OpEx) for data centers. While liquid cooling is designed to manage this, the higher power draw contributes to the increased TCO and could be a limiting factor for some data center infrastructures not yet equipped for such densities.
    *   **Consensus Check:** Widely known, but the direct comparison of 1200W vs 700W per chip for TCO implications is less frequently highlighted by NVIDIA.

*   **Snippet:** "Nvidia's latest MLPerf Training 5.0 results show its Blackwell GB200 accelerators sprinting through record time-to-train scores, showing how the company's rack-scale “AI-factory” design can turn raw compute into deployable models more quickly than before. Using 34 72-GPU NVLink domains combined into a 2,496-GPU cluster, the system sustained 1,829 teraflops (roughly 1.8 petaflops) of measured application throughput while maintaining 90 percent scaling efficiency. Additionally, it was the only submission to complete the suite's new 405-billion-parameter Llama-3.1 workload, making this Nvidia's most ambitious MLPerf showing to date."
    *   **Date:** June 4, 2025
    *   **Source:** HPCwire, "Blackwell GPUs Lift Nvidia to the Top of MLPerf Training Rankings"
    *   **Impact:** High. These results confirm the Blackwell platform's leading performance in large-scale AI training, especially for complex models like Llama 3.1 405B. This validates NVIDIA's architectural advancements and its "AI-factory" concept, which is crucial for long-term demand.
    *   **Consensus Check:** Widely known. This is a key part of NVIDIA's public narrative and expected performance.

*   **Snippet:** "CoreWeave (CRWV), NVIDIA, and IBM achieved record-breaking MLPerf Training v5.0 results using 2,496 NVIDIA GB200 Blackwell GPUs on CoreWeave's AI cloud platform. The submission represents the largest NVIDIA GB200 NVL72 cluster ever benchmarked, 34x larger than other cloud provider submissions. The cluster completed training the Llama 3.1 405B model in just 27.3 minutes, demonstrating 2x faster training performance compared to similar-sized clusters."
    *   **Date:** June 4, 2025
    *   **Source:** Stock Titan, "CoreWeave, NVIDIA and IBM Submit Largest-Ever MLPerf Results on NVIDIA GB200 Grace Blackwell Superchips"
    *   **Impact:** High. This partner disclosure from CoreWeave, a key NVIDIA customer, provides real-world validation of GB200's performance at scale. It reinforces the MLPerf results and demonstrates early customer success, which is critical for driving further adoption.
    *   **Consensus Check:** Widely known, as it's part of the MLPerf results announcement, but the specific details from a cloud provider add weight.

*   **Snippet:** "The GB200's 47% improvement in TFLOPS per GPU Watt vs. the H100 is helpful – but again hardly anything to write home about without further quantization of models, and as such is certainly not enough to deliver the 30x inference performance showcased in the keynote address."
    *   **Date:** May 27, 2025
    *   **Source:** Continuum Labs, "TCO of NVIDIA GPUs and falling barriers to entry"
    *   **Impact:** Medium. This analyst perspective highlights that while there are perf/W gains, the headline 30x inference performance is largely achieved through system-level integration and advanced quantization (FP4), rather than solely raw per-GPU power efficiency. This nuanced view could temper expectations around simple "drop-in" efficiency improvements.
    *   **Consensus Check:** Overlooked. NVIDIA's marketing often emphasizes the headline "30x" figure, while the underlying factors and per-GPU efficiency are less scrutinized by the broader market.

**Contradictions and Gaps:**

The most significant contradiction lies between the impressive MLPerf Training 5.0 results (June 2025) showcasing GB200's record-breaking performance and scalability, and the August 20, 2025, SemiAnalysis report stating that "currently there are no large-scale training runs done yet on GB200 NVL72 as software continues to mature and reliability challenges are worked through."

This creates a critical gap: are the MLPerf results indicative of production readiness, or are they highly optimized, controlled benchmarks that don't reflect the current state of large-scale customer deployments? If the SemiAnalysis report is accurate, it suggests that the real-world ramp of GB200 for frontier-scale training is slower than the market might perceive based on the benchmark headlines. This could lead to a more conservative outlook for Q3 FY2026 and beyond, as customers may delay large-scale GB200 deployments until software and reliability issues are fully resolved.