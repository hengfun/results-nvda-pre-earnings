# Research Query: Do customer case studies state training runs migrating to GB200 with performance data?,ISV blogs; engineering posts; arXiv appendices,migrate to GB200 training throughput cost per token case study
**Generated:** Wednesday, August 27, 2025 at 11:57:32 AM
**Model:** gemini-2.5-flash

## Search Queries Used
- GB200 training performance data case study after:2025-05-27
- GB200 migration case study throughput cost per token after:2025-05-27
- NVIDIA GB200 customer case study performance data after:2025-05-27
- NVIDIA GB200 ISV blog performance after:2025-05-27
- NVIDIA GB200 engineering post performance after:2025-05-27

## Sources Referenced
- [nvidia.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGBqb6-XtAnMgYEvkyfL7WyK8s9l1qc9wuk9To7b-ZBB_JnIwg6SMmQxoa1qsmArMuFm_eBd4Y6PMBSXNOIBhNnKwXhz8Kw--0JUmMsht8ksA5o0iSQIFTQ2N7m7lClPPEF9jlTZyRwkWXvD1ekHi-txk8YM9tDcHC_OC1cyFNtbKpzEjCkHDZThBc44z4frFQiIb4i3xQXZC5tCnf2PYusJUPruXBFJNiWF0ZT)
- [coreweave.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHWpNk6ShIK8WmxVV0d30lM_SDgbdMfBeR9AL1TpurYC1skDXTS2nUqnzvJ9PcMFns3Wr6-f5YqUpL_QkBze96ZltFsjd0PZZtS4MmNBKSVdIuxzhdpGBtiUHcGSPBAoJSO3WnqSTsDWWvTLgbnG5f5k7Y_d1XJND3_N5O5c9qOq0vj9kBJ_JtSTxIXNm0NYqgVpN5KKxo1zHTAXDDqdN4Z2hoKubnpWBQDmqCN)
- [uncoveralpha.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEaDoDbJVasYmj0ynIWTi-Q2hKYLIoLP-fO9vSRNQhqzmtEXUxRk23N1zdsX5j9ciMQDK-JYrVmnehs4ZJ68sz81-4WM5HG4erdcl8ogb_QzVcg-FDgftubzpdSElgWmE6dEEElphFG7IzhsCkf7u1WBSlqYBHuAMOy)
- [semianalysis.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFIX5hj_wj16wJ3PuG2Hq6O3lnOBvtl1MN_WIHI-kj2MSZ1-IjCZ0Z7AOTtKsz6_MICGVbYZS7C6rXXjsOsqSzJxFgUpJx6xTaHQERcIEEw5CYosD5dGss06xwlPk3bPrwZP6gTgJ-KE3UpKnBajWI1wOAk4OpT-8m0uJl6h63ZEdA6pwmG8Q==)
- [deepl.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFeG3eb6sYdxW6m0puXDESHOL4HZGjk7TgV5zTqyB2E9JAVkYT9vo-LoH_L2SFmZyi485PI4Bqcku9Dcq6YuvqCGxGX1UKqwYIKG9x-LriTBtQONa9H1JxqTxyXWRUBwVSPAAFXW3MFmSSsg6whekTn2cpHVvzbJS7p4Ot4Gy0=)
- [nvidia.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGzqD3xP3_RimnSUwNecZNXIrQyllt7FEgX2GU4OIOPq9LJD5LkSSMaVPtw-PE0bD_tJnGn4Qv4MF1SSzUxge6NGnKnJ-VkbqJWuIlJV05SNyazO7VycpkslXOBHRuiUDM34R9LAygqm0uns1UT3gMIFiQs99LB7TO9_wHUCo36DX3uLwsEFE2fShfAfUAJeQLAWrvhwEY3k6IsF7eB3z2vVI3dHLtYfAyldsxdQVWaJp4AcjS_YAUArT2U2sU6ND5l8vw5kAdjS4vWBCOcxw==)
- [cursor-ide.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF-Dv-eZ47I4McfRCMoC_Nhcb8oLrhRaguLIjpA4aMEXhAIFv96Lc0qZ8ZgXKw8_oDIX88equOuJvyAM_bLFzDd8RH3msD-tuoKer31wKAZxzEWS0DsUOMWCcanrXLs4rUt3FRLBqy2MwPpzMs5ehcESfuIiNc=)
- [comsoc.org](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE6kfpJRBCR9VsXzXSBr362R3Z11TIsf0RpNepgMX5Y1CsPew6Lp3Jp0Y2Xeq_idgeGYdr0JKa9381tUgj4ETvQYYGBi4LxPJgF0BK_YXn65szNyId8Ryi_SVjC-Xn8)
- [hpcwire.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF8ByK045Zyjp8QXDtvZ00Y1w4iDUunCRfB5OSV-Wvj6XZNtbXUc9TcLHpEV7yiTEcDhE3iwSQ7kHnEqOg9HsWE6ZLrH3HLa5x-2-mmjwulA35b2hrk0rWXR1iyV8MUcMhNB_S9-pgwM0hH3CcAo73IB7YKhwd4d2cqYHc-e2zthNpYMN3-EYtUkKSraoBO-MtL30xvX92R0_08CbTxXfY=)

---

## Analysis

As a pre-earnings research analyst for NVIDIA (NVDA), the following material, non-consensus information regarding the GB200 platform has been extracted from recent web content. This information is likely to impact NVIDIA's Q2 FY2026 earnings report or forward-looking guidance, particularly concerning customer adoption, performance benchmarks, cost-efficiency, and competitive landscape.

---

### Structured Findings:

**1. GB200 NVL72 Demonstrates Significant Training Performance Gains in MLPerf**

*   **Snippet:** "On the R-GAT training test, based on R-GAT, NVIDIA submissions using GB200 NVL72 delivered a 2.2x improvement in per-GPU performance compared to NVIDIA submissions using the H100 Tensor Core GPU. ... GB200 NVL72 to train 2.2x faster compared to Hopper when 512 GPUs are used to run the Llama 3.1 405B benchmark. GB200 NVL72 achieves up to 1,960 TFLOPS of training throughput with Llama 3.1 405B Pretraining benchmark. ... In addition to delivering up to 2.6x more performance per GPU compared to Hopper, GB200 NVL72 submissions also delivered excellent performance at scale, and demonstrated near-linear scaling efficiency on the demanding LLM pretraining benchmark based on Llama 3.1 405B."
*   **Date:** June 4, 2025
*   **Source:** NVIDIA Blog post referencing MLPerf Training v5.0 results, [https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGBqb6-XtAnMgYEvkyfL7WyK8s9l1qc9wuk7To7b-ZBB_JnIwg6SMmQxoa1qsmArMuFm_eBd4Y6PMBSXNOIBhNnKwXhz8Kw--0JUmMsht8ksA5o0iSQIFTQ2N7m7lClPPEF9jlTZyRwkWXvD1ekHi-txk8YM9tDcHC_OC1cyFNtbKpzEjCkHDZThBc44z4frFQiIb4i3xQXZC5tCnf2PYusJUPruXBFJNiWF0ZT]
*   **Impact:** High. These are official benchmark results showcasing substantial performance improvements (2.2x to 2.6x) over the previous generation (H100/Hopper) for critical AI training workloads like LLM pretraining and graph neural networks. This validates the GB200's technical superiority and justifies its premium pricing, potentially driving strong demand and revenue.
*   **Consensus Check:** Widely known, as MLPerf results are public and often highlighted by NVIDIA. However, the specific magnitude of improvement across various benchmarks provides concrete data points that analysts will use to model future performance and adoption.

**2. Mistral AI Achieves 2.5x Faster Training with GB200 NVL72 on CoreWeave**

*   **Snippet:** "Mistral AI was among the first to gain access to NVIDIA GB200 NVL72 racks. This opportunity unlocked an unprecedented level of computing power their teams could leverage to run training and inference jobs—accelerating both overall productivity and time-to-market. With the support of CoreWeave infrastructure, Mistral AI was able to train models 2.5x faster than previously experienced on NVIDIA H200s—and also vastly exceeded the performance, efficiency, and speed they experienced on NVIDIA H100s."
*   **Date:** August 13, 2025
*   **Source:** Mistral AI Case Study - CoreWeave, [https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHWpNk6ShIK8WmxVV0d30lM_SDgbdMfBeR9AL1TpurYC1skDXTS2nUqnzvJ9PcMFns3Wr6-f5YqUpL_QkBze96ZltFsjd0PZZtS4MmNBKSVdIuxzhdpGBtiUHcGSPBAoJSO3WnqSTsDWWvTLgbnG5f5k7Y_d1XJND3_N5O5c9qOq0vj9kBJ_JtSTxIXNm0NYqgVpN5KKxo1zHTAXDDqdN4Z2hoKubnpWBQDmqCN]
*   **Impact:** High. This is a crucial real-world customer validation from a prominent AI startup, Mistral AI, demonstrating a 2.5x speedup in training over H200s. This directly supports NVIDIA's claims of GB200's performance benefits and indicates strong early adoption among leading AI innovators, which could translate into significant future orders.
*   **Consensus Check:** Overlooked by some, as it's a partner case study rather than a broad NVIDIA announcement. While the general idea of GB200 being faster is known, specific customer testimonials with concrete numbers are valuable non-consensus data points.

**3. Initial GB200 NVL72 Performance per TCO and Reliability Challenges**

*   **Snippet:** "GB200 NVL72 first shipment started in mid-February this year. By May 2025, the GB200 NVL72's performance per TCO had not surpassed that of the H100; however, by July, Semianalysis began to see the performance per TCO reach 1.5x that of the H100, which is close to the target range. ... Currently there are no large-scale training runs done yet on GB200 NVL72 as software continues to mature and reliability challenges are worked through. This means that Nvidia's H100 and H200 as well as Google TPUs remain the only GPUs that are today being successfully used to complete frontier-scale training. As it stands today, even the most advanced operators at frontier labs and CSPs are not yet able to carry out mega training runs on the GB200 NVL72."
*   **Date:** August 22, 2025; August 20, 2025
*   **Source:** UncoverAlpha (citing SemiAnalysis), [https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEaDoDbJVasYmj0ynIWTi-Q2hKYLIoLP-fO9vSRNQhqzmtEXUxRk23N1zdsX5j9ciMQDK-JYrVmnehs4ZJ68sz81-4WM5HG4erdcl8ogb_QzVcg-FDgftubzpdSElgWmE6dEEElphFG7IzhsCkf7u1WBSlqYBHuAMOy]; SemiAnalysis, [https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFIX5hj_wj16wJ3PuG2Hq6O3lnOBvtl1MN_WIHI-kj2MSZ1-IjCZ0Z7AOTtKsz6_MICGVbYZS7C6rXXjsOsqSzJxFUpxJx6xTaHQERcIEEw5CYosD5dGss06xwlPk3bPrwZP6gTgJ-KE3UpKnBajWI1wOAk4OpT-8m0uJl6h63ZEdA6pwmG8Q==]
*   **Impact:** High. This is a critical non-consensus finding. While performance gains are evident, the initial challenges with GB200 NVL72's performance per Total Cost of Ownership (TCO) not surpassing H100 until July, and the ongoing software maturity and reliability issues preventing large-scale frontier training runs, suggest a slower ramp-up than some might anticipate. This could temper near-term revenue expectations for GB200 and potentially shift some demand back to H100/H200 or even Google TPUs for immediate large-scale needs.
*   **Consensus Check:** Highly overlooked. NVIDIA's public messaging focuses on peak performance. This detailed analysis from SemiAnalysis provides a more nuanced and potentially bearish view on the immediate, widespread deployment of GB200 for the most demanding training workloads.

**4. DeepL's GB200 SuperPOD Deployment Highlights Architectural Leap for Large-Scale Training**

*   **Snippet:** "With our current DGX SuperPOD with DGX H100 systems leveraging NVIDIA's Hopper GPU architecture, we have islands of 8 GPUs connected to function as a single GPU. With the DGX GB200, the pooling factor increases to 72 GPUs, a factor of nine. Add in the fact that each of these GPUs itself has a higher Video Random Access Memory (VRAM), a greater memory bandwidth and can execute more Floating Point Operations per second (FLOPS), the other most important constraint when training models, and you quickly get a sense of the step-change involved."
*   **Date:** July 9, 2025
*   **Source:** DeepL Tech Blog, [https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFeG3eb6sYdxW6m0puXDESHOL4HZGjk7TgV5zTqyB2E9JAVkYT9vo-LoH_L2SFmZyi485PI4Bqcku9Dcq6YuvqCGxGX1UKqwYIKG9x-LriTBtQONa9H1JxqTxyXWRUBwVSPAAFXW3MFmSSsg6whekTn2cpHVn_qjI70qaW25Mf2Zt2WbJrXdHvA==]
*   **Impact:** Medium-High. This customer perspective from DeepL, a leading AI company, underscores the architectural leap of GB200, particularly the increase in GPU pooling factor from 8 (H100) to 72. This is a significant advantage for training massive models that require extensive inter-GPU communication, reinforcing the long-term value proposition of GB200 for frontier AI development.
*   **Consensus Check:** Partially known. While NVIDIA promotes the NVLink capabilities, a customer articulating the "factor of nine" increase in GPU pooling provides a clearer, more tangible understanding of the architectural shift and its implications for model training.

**5. GB200 NVL72 Achieves 1.5 Million Tokens/Second for OpenAI GPT-OSS Inference**

*   **Snippet:** "At launch, based on early performance measurements, a single GB200 NVL72 rack-scale system is expected to serve the larger, more computationally demanding gpt-oss-120b model at 1.5 million tokens per second, or about 50,000 concurrent users. Blackwell features many architectural capabilities that accelerate inference performance. ... The model's 128K token context window and 1.5 million tokens per second throughput on NVIDIA GB200 systems establish new benchmarks for open-source performance."
*   **Date:** August 5, 2025; August 2025
*   **Source:** NVIDIA Blog, [https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGzqD3xP3_RimnSUwNecZNXIrQyllt7FEgX2GU4OIOPq9LJD5LkSSMaVPtw-PE0bD_tJnGn4Qv4MF1SSzUxge6NGnKnJ-VkbqJWuIlJV05SNyazO7VycpkslXOBHRuiUDM34R9LAygqm0uns1UT3gMIFiQs99LB7TO9_wHUCo36DX3uLwsFFE2fShfAfUAJeQLAWrvhwEY3k6IsF7eB3z2vVI3dHLtYfAyldsxdQVWaJp4AcjS_YAUArT2U2sU6ND5l8vw5kAdjS4vWBCOcxw==]; Cursor IDE, [https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF-Dv-eZ47I4McfRCMoC_Nhcb8oLrhRaguLIjpA4aMEXhAIFv96Lc0qZ8ZgXKw8_oDIX88equOuJvyAM_bLFzDd8RH3msD-tuoKer31wKAZxzEWS0DsUOMWCcanrXLs4rUt3FRLBqy2MwPpzMs5ehcESfuIiNc]
*   **Impact:** High. This provides concrete, high-throughput inference performance data for a significant open-source LLM (GPT-OSS-120B) on GB200. The 1.5 million tokens per second figure is a strong indicator of GB200's capabilities for serving large-scale AI applications, which is becoming increasingly important for revenue generation as models move from training to deployment. The mention of "zero-cost AI" with open-source models also highlights the importance of efficient hardware for cost-effective deployment.
*   **Consensus Check:** While NVIDIA generally promotes inference capabilities, this specific data point for a widely used open-source model and the direct link to cost-effectiveness for enterprises is a valuable, more granular insight.

**6. Huawei's CloudMatrix 384 Poses Competitive Threat to GB200 NVL72**

*   **Snippet:** "Dylan Patel, founder of semiconductor research group SemiAnalysis, said in an April article that Huawei now had AI system capabilities that could beat Nvidia's AI system. The CloudMatrix 384 incorporates 384 of Huawei's latest 910C chips and outperforms Nvidia's GB200 NVL72 on some metrics, which uses 72 B200 chips, according to SemiAnalysis. The performance stems from Huawei's system design capabilities, which compensate for weaker individual chip performance through the use of more chips and system-level innovations, SemiAnalysis said."
*   **Date:** July 31, 2025
*   **Source:** IEEE ComSoc Technology Blog (citing SemiAnalysis), [https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE6kfpJRBCR9VsXzXSBr362R3Z11TIsf0RpNepgMX5Y1CsPew6Lp3Jp0Y2Xeq_idgeGYdr0JKa9381tUgj4ETvQYYGBi4LxPJgF0BK_YXn65szNyId8Ryi_SVjC-Xn8]
*   **Impact:** High. This is a significant non-consensus and potentially negative finding. The emergence of Huawei's CloudMatrix 384, with claims of outperforming GB200 NVL72 on "some metrics" through system-level innovations, introduces a direct and formidable competitor. This could impact NVIDIA's market share, especially in regions where Huawei has a strong presence, and put pressure on pricing or future guidance.
*   **Consensus Check:** Highly overlooked. While general competition is always a factor, specific claims of a competitor's system outperforming NVIDIA's latest flagship on certain metrics are not widely discussed in mainstream NVIDIA coverage and represent a material, non-consensus risk.

---

### Contradictions and Gaps:

*   **Contradiction in TCO vs. Raw Performance:** While MLPerf and customer testimonials highlight significant raw performance gains for GB200, SemiAnalysis points out that the performance per TCO was initially not superior to H100 and that reliability challenges are slowing large-scale deployments. This suggests a gap between theoretical peak performance and practical, cost-effective deployment in the very near term.
*   **Lack of Detailed Cost-Per-Token Case Studies:** While "cost per token" is mentioned as a key metric, detailed customer case studies with specific cost savings or comparisons on a per-token basis are still limited. The Cursor IDE snippet mentions "zero API fees" for open-source models on GB200, which implies cost efficiency, but granular data is scarce.
*   **Limited Information on GB200 Training Migrations from H200:** Most comparisons are against H100. While Mistral AI mentions H200, more detailed data on migrations from H200 to GB200 for training would be beneficial, especially given H200's increased memory.
*   **Absence of Power Consumption Data:** The MLPerf results explicitly noted the absence of power results for full GB200 NVL72 configurations, which is a significant gap in understanding the true operational cost and efficiency.
*   **Focus on LLMs:** While LLMs are critical, more diverse case studies beyond language models would provide a broader view of GB200's impact across different AI domains.

This analysis suggests that while GB200 offers compelling performance, particularly for inference and certain training workloads, the initial TCO and reliability hurdles, coupled with emerging competition, warrant careful consideration in NVIDIA's upcoming earnings call.