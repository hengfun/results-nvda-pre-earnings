# Research Query: Are foundation model labs disclosing cluster specs implying NVIDIA share of training compute?,Open-source lab blogs; arXiv appendices; GitHub READMEs,foundation model cluster spec NVIDIA GPU count share training compute
**Generated:** Wednesday, August 27, 2025 at 11:57:53 AM
**Model:** gemini-2.5-flash

## Search Queries Used
- foundation model cluster specs NVIDIA GPU count training compute after:2025-05-27
- LLM training cluster NVIDIA A100 H100 share after:2025-05-27
- open-source AI lab GPU infrastructure NVIDIA blogs after:2025-05-27
- foundation model hardware NVIDIA market share training after:2025-05-27
- AI research lab GPU cluster specifications NVIDIA after:2025-05-27

## Sources Referenced
- [neptune.ai](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH5_GwTmXrLqRNbEsl_346lt_kAnwRri3DukzSbHknLtbnvra6oIkeid4zUgq4aD65LcUfWDVIeZxNIrJZFgQlHeLCJ8I79KnNfQV6dHUx9_gbSMUfnJHY5wBJoJOyrZymIajV9nNsOA30I6_xNoMnNJIJy2h0rbg==)
- [wandb.ai](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF5BO0nGZ23bfVizQYFH5_shkcL7Th1LdbNzNx26ZwQabKj6P2E2RlXiss0u84pEeRxDc7rVYwvVnoudjfM_osfFl-JNGYM-2ZSyGirQ-PN67dCP4MPPrzwdL0KImwafZaejCgaOTXH_mAxdgTYedTjqT5FzFBF88OImFsosknPK-yBYtjG76oW3Dz7dH4-Pe0hHPXGP00aNCdTsRA9pWkJohSq9fQM02uFX7DMIBT1XhL41-z9Sg6fBjgv_VC1QhemFw==)
- [google.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG5Y4iz32FH6B1SqtjzI1914Yh-UkZgPx4JHToCE4EsY83dkKBeYsxW9Dwd2Cf9OjgUS3RN1FPXtxsz5BEJkfLKXIiXAhhA6IqESpiqfXFkIu13z-hYdsFbNK0Bp53WD4PoIoabbmvS-4My6N8E)
- [vast.ai](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFzh5w7Nq2zFz0kiTCXqPmin3W0frzE4VHHUrI0iN6WP4g-yIJP4VNdi-2K7wvPeRvezMZOXKtdn2bZNVuo9HItc1hQankoUOFx2OT3aa918ppOXuhgtbV_U_tl5-_qYp9-vmh16j2OQXsyrg==)
- [nasdaq.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH0ZssVHgAE5MP_dlgrhK5ZVhJri9OwA0oM1p4iSLqFVNHCvrp1ab67rKWN4iljp55V9RVVCUy8xDYFpt9fSfMJuJuZBW3hNgGswpViejQWPO8OYgX31YQv4tEf9PN_PyhM0ar-ufCPec-Laab9gDxvhI9BTj2DLriVWWO5Wo8_z-e1E0GAUPf1WlS8f58eYLa1CFdZBn_Q6vcQAE1EannxAhJXN2jl-Qpy-OFPY6MjF7t_K1Xa)
- [epoch.ai](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG1i5-OTrCIy8bnPuTL6KiUUo7f7wTXDGm3Yi2zx1bjrLPShiG6nh1Hs0-2ATsOrQVSpnwKX3Ti3mgLHjMmYEnVPFF6MTPyL4LoFARAB7jQLjo3pn_hrmr4ilURZEQ2tTPDs6dKyBjNR9ecEw==)
- [nvidia.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFewbkoG4LikLKwy1qxXqHcQTK5ZJ_CrTAXkv6_LE9o-cF27HFnIOuicMs9uoMjgUWVUd2q-TX7jVvP9AQP_wtoOnU9mTkxIRB3TdBdEzpd63_17jurzYQNawR-ZJB3YVQEknnCnrxB_5IXtS96ZIAq-fUkO5l0gLUD8QUyvt77pdS66WRvTbAzSUr4-XihkXJqmyqA6E9hoEcx8BD_2TJgEOrpHJeJloP1FxDQpg==)
- [ts2.tech](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEOrCkgxHgmSnXRSQpqmz3idcZ6yMIiK_xeVNAnqKdn8HiMM8e3UKcGUpCMv8mev-r83Eqnjr3dB9Rgo7kex5NUm9rTzXntk3Ro7N7CFYlQgyvaPkTs8rxQaBlMc9p9ARValSz_1TQ7AT3hd7YxOJAMG5pLdedD81ziHSTi0XsOG6IRdZt-IN_xpJKhawocXSHwiIrmJpdczGN2a1NByhMUQoj-1lj5rEOOPbp9D1Ti)

---

## Analysis

As NVIDIA prepares to report its Q2 FY2026 earnings, a deep dive into recent disclosures from foundation model labs and related industry reports reveals several material, non-consensus insights regarding the company's market position in AI training compute. While NVIDIA continues to dominate, specific data points highlight emerging competitive pressures, the rapid adoption of its latest architectures, and the growing, yet often opaque, landscape of custom AI chip development by hyperscalers.

The research indicates that NVIDIA's Blackwell and Grace Blackwell (GB200) architectures are quickly being deployed in high-performance clusters and cloud offerings, underscoring continued strong demand for its cutting-edge GPUs. However, the increasing efforts by major customers like Meta and Microsoft to develop in-house AI training chips present a long-term challenge to NVIDIA's market share, particularly for future guidance. Furthermore, while NVIDIA's overall market share remains high, specific competitive gains by AMD in the data center segment and the limited visibility into custom AI chip deployments suggest a more nuanced competitive environment than often assumed.

Here are the key findings:

---

**Structured Findings:**

-   **Snippet:** "Nvidia continues to dominate the GPU market and has established a de facto industry standard with its CUDA framework. However, AMD has tripled its revenue from equipping data centers between Q2/2023 and Q4/2024, with half of the world's top 10 HPC clusters as of November 2024 running on their Instinct GPUs. Intel plays a significant role in the data center GPU market as well."
    -   **Date:** 2025-06-05
    -   **Source:** State of Foundation Model Training Report 2025 - neptune.ai, https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH5_GwTmXrLqRNbEsl_346lt_kAnwRri3DukzSbHknLtbnvra6oIkeid4zUgq4aD65LcUfWDVIeZxNIrJZFgQlHeLCJ8I79KnNfQV6dHUx9_gbSMUfnJHY5wBJoJOyrZymIajV9nNsOA30I6_xNoMnNJIJy2h0rbg==
    -   **Impact:** Medium. While NVIDIA's dominance is widely known, AMD's specific growth in data center revenue and presence in a significant portion of top HPC clusters indicates a tangible, albeit smaller, competitive threat that could impact NVIDIA's long-term market share projections.
    -   **Consensus Check:** Overlooked. While AMD is a known competitor, the specific figures of tripled revenue and presence in half of the top 10 HPC clusters are more granular and less frequently highlighted in general market commentary.

-   **Snippet:** "GPU Configuration: 72× NVIDIA Blackwell Ultra GPUs, fully interconnected via NVLink and NVSwitch. Each GPU provides up to ~15 PF4 (FP4 precision) peak compute (approx 15 petaflops) and contains 288 GB of HBM3e memory on board. The GPUs are arranged to work in concert as one giant accelerator, with NVLink connecting all 72 GPUs into a single high-speed cluster."
    -   **Date:** 2025-07-04
    -   **Source:** NVIDIA GB300 NVL72: A leap in AI performance and efficiency | genai-research - Wandb, https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF5BO0nGZ23bfVizQYFH5_shkcL7Th1LdbNzNx26ZwQabKj6P2E2RlXiss0u84pEeRxDc7rVYwvVnoudjfM_osfFl-JNGYM-2ZSyGirQ-PN67dCP4MPPrzwdL0KImwafZaejCgaOTXH_mAxdgTYedTjqT5FzFBF88OImFsosknPK-yBYtjG76oW3Dz7dH4-Pe0hHPXGP00aNCdTsRA9pWkJohSq9fQM02uFX7DMIBT1XhL41-z9Sg6fBjgv_VC1QhemFw==
    -   **Impact:** High. This details a specific, large-scale deployment of NVIDIA's latest Blackwell Ultra GPUs (part of the GB300 NVL72 system), indicating strong early adoption and demand for their most advanced hardware for foundation model training. This directly supports NVIDIA's data center revenue growth.
    -   **Consensus Check:** Widely known, but specific details are valuable. The general availability of Blackwell is known, but a concrete example of a 72-GPU Blackwell Ultra cluster provides specific evidence of its integration into high-end AI research and development.

-   **Snippet:** "A4X machine types use NVIDIA GB200 Grace Blackwell Superchips ( nvidia-gb200 ) and are ideal for foundation model training and serving. A4X is an exascale platform based on NVIDIA GB200 NVL72. Each machine has two sockets with NVIDIA Grace CPUs with Arm Neoverse V2 cores. These CPUs are connected to four NVIDIA B200 Blackwell GPUs with fast chip-to-chip (NVLink-C2C) communication. ... GPU count, 4, GPU memory‡ (GB HBM3e), 720."
    -   **Date:** Not explicitly stated, but the document is from Google Cloud and references current offerings.
    -   **Source:** GPU machine types | AI Hypercomputer - Google Cloud, https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG5Y4iz32FH6B1SqtjzI1914Yh-UkZgPx4JHToCE4EsY83dkKBeYsxW9Dwd2Cf9OjgUS3RN1FPXtxsz5BEJkfLKXIiXAhhA6IqESpiqfXFkIu13z-hYdsFbNK0Bp53WD4PoIoabbmvS-4My6N8E
    -   **Impact:** High. Google Cloud's offering of A4X machine types with NVIDIA GB200 Grace Blackwell Superchips directly demonstrates hyperscaler adoption of NVIDIA's newest, highest-performance platforms for foundation model training and serving. This is a strong indicator of continued revenue from major cloud providers.
    -   **Consensus Check:** Widely known. Hyperscaler adoption of new NVIDIA architectures is generally expected, but the explicit mention of GB200 in Google Cloud's offerings reinforces this trend.

-   **Snippet:** "NVIDIA DGX B200 – A universal AI supercomputer built on the Blackwell architecture, equipped with eight GPUs and 1.4 TB of GPU memory. ... We also recently reached an exciting milestone: we now have over one thousand RTX 5090 GPUs available on Vast – 1,180 and counting."
    -   **Date:** 2025-06-30
    -   **Source:** June 2025 Product Update - Vast AI, https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFzh5w7Nq2zFz0kiTCXqPmin3W0frzE4VHHUrI0iN6WP4g-yIJP4VNdi-2K7wvPeRvezMZOXKtdn2bZNVuo9HItc1hQankoUOFx2OT3aa918ppOXuhgtbV_U_tl5-_qYp-vmh16j2OQXsyrg==
    -   **Impact:** Medium-High. The availability of DGX B200 (Blackwell architecture) indicates continued demand for high-end data center solutions. More notably, the specific number of 1,180 RTX 5090 GPUs on Vast.ai suggests strong demand and supply for NVIDIA's high-end consumer/prosumer GPUs in the AI market, potentially capturing a segment of the market not always fully accounted for in enterprise-focused analyses.
    -   **Consensus Check:** Overlooked. While DGX B200 is expected, the specific, large number of RTX 5090s available on a cloud provider like Vast.ai for AI workloads is a more granular data point that might be missed by analysts focusing solely on data center-grade GPUs.

-   **Snippet:** "That said, some of Nvidia's biggest customers, like Meta Platforms (NASDAQ: META) and Microsoft (NASDAQ: MSFT), are wary of becoming overly reliant on Nvidia for their AI training hardware needs. Meta, for example, is taking its Meta Training and Inference Accelerator platform and applying it to more and more generative AI applications. The next version of its chip is designed to replace Nvidia chips in AI training for its Llama foundation model. It's already using its own chips in some AI inference cases."
    -   **Date:** 2025-07-20
    -   **Source:** Nvidia Just Topped a $4 Trillion Market Cap, but a Different Artificial Intelligence (AI) Giant Is Headed to $4.5 Trillion, According to a Certain Wall Street Analyst | Nasdaq, https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH0ZssVHgAE5MP_dlgrhK5ZVhJri9OwA0oM1p4iSLqFVNHCvrp1ab67rKWN4iljp55V9RVVCUy8xDYFpt9fSfMJuJuZBW3hNgGswpViejQWPO8OYgX31YQv4tEf9PN_PyhM0ar-ufCPec-Laab9gDxvhI9BTj2DLriVWWO5Wo8_z-e1E0GAUPf1WlS8f58eYLa1CFdZBn_Q6vcQAE1EannxAhJXN2jl-Qpy-OFPY6MjF7t_K1Xa
    -   **Impact:** High. This is a critical non-consensus point. The explicit mention of Meta and Microsoft developing their own chips to replace NVIDIA's for AI training, particularly for foundation models like Llama, signals a significant long-term risk to NVIDIA's market share and could impact forward-looking guidance.
    -   **Consensus Check:** Overlooked. While the general idea of hyperscalers developing custom silicon is known, the specific intent to *replace* NVIDIA chips for *training* Llama foundation models is a more direct and material threat that may not be fully priced into current expectations.

-   **Snippet:** "We estimate that we cover 10-20% of GPU clusters by computing power as of early 2025. This includes roughly 20-37% of NVIDIA H100s, 12% of A100s, and 18% of AMD MI300Xs. Meanwhile, we estimate we cover less than 4% of Google's TPUs and very few custom AI chips designed by AWS, Microsoft, or Meta."
    -   **Date:** Not explicitly stated, but "early 2025" for coverage estimate.
    -   **Source:** GPU Clusters Documentation | Epoch AI, https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG1i5-OTrCIy8bnPuTL6KiUUo7f7wTXDGm3Yi2zx1bjrLPShiG6nh1Hs0-2ATsOrQVSpnwKX3Ti3mgLHjMmYEnVPFF6MTPyL4LoFARAB7jQLjo3pn_hrmr4ilURZEQ2tTPDs6dKyBjNR9ecEw==
    -   **Impact:** High. This provides specific, albeit partial, market share estimates for NVIDIA's H100s and A100s within a surveyed subset of clusters. Crucially, it highlights a significant gap in coverage for custom AI chips from major hyperscalers (AWS, Microsoft, Meta), implying that the true competitive landscape, particularly concerning in-house solutions, is less transparent and potentially more challenging for NVIDIA than commonly perceived.
    -   **Consensus Check:** Highly overlooked. This snippet directly addresses the "gaps" in market understanding and provides specific, granular (though limited) data on market share and competitive visibility.

-   **Snippet:** "Submissions this round using NVIDIA GB200 NVL72-based systems, which are powered by the NVIDIA Blackwell platform, delivered outstanding performance in MLPerf Training v5. 0. In addition to delivering up to 2.6x more performance per GPU compared to Hopper, GB200 NVL72 submissions also delivered excellent performance at scale, and demonstrated near-linear scaling efficiency on the demanding LLM pretraining benchmark based on Llama 3.1 405B."
    -   **Date:** 2025-06-04
    -   **Source:** NVIDIA Blackwell Delivers up to 2.6x Higher Performance in MLPerf Training v5.0, https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFewbkoG4LikLKwy1qxXqHcQTK5ZJ_CrTAXkv6_LE9o-cF27HFnIOuicMs9uoMjgUWVUd2q-TX7jVvP9AQP_wtoOnU9mTkxIRB3TdBdEzpd63_17jurzYQNawR-ZJB3YVQEknnCnrxB_5IXtS96ZIAq-fUkO5l0gLUD8QUyvt77pdS66WRvTbAzSUr4-XihkXJqmyqA6E9hoEcx8BD_2TJgEOrpHJeJloP1FxDQpg==
    -   **Impact:** High. While NVIDIA's performance leadership is expected, the specific benchmarks from MLPerf Training v5.0, showing 2.6x performance improvement per GPU for Blackwell (GB200 NVL72) over Hopper and near-linear scaling for large LLM pretraining (Llama 3.1 405B), provide concrete evidence of NVIDIA's continued technological advantage. This reinforces the value proposition for customers investing in their latest hardware.
    -   **Consensus Check:** Widely known, but specific data is valuable. The general idea of Blackwell outperforming Hopper is consensus, but the exact performance multipliers and scaling efficiency on a demanding LLM benchmark offer specific, material validation of NVIDIA's technological lead.

---

**Contradictions and Gaps:**

*   **Contradiction:** There isn't a direct contradiction, but rather a tension between NVIDIA's reported market dominance (90%+ share for AI training) and the growing efforts by hyperscalers (Meta, Microsoft) to develop their own chips to *replace* NVIDIA's for training foundation models. This suggests that while NVIDIA currently holds a commanding lead, the competitive landscape is evolving rapidly, and future market share is not guaranteed.
*   **Gaps:** A significant gap identified by Epoch AI is the limited visibility into custom AI chips designed by AWS, Microsoft, or Meta. This means that while NVIDIA's share of *known* GPU clusters is high, the market share of these in-house solutions, which are directly competitive, is largely unquantified and could be larger than generally assumed. This lack of transparency makes it difficult to fully assess NVIDIA's long-term competitive positioning. There is also limited specific, granular data from open-source lab blogs or arXiv appendices detailing exact NVIDIA GPU counts for *new* foundation model training runs, beyond general statements of using A100s or H100s.